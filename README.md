# Projet Web Scraping E-commerce

Ce projet impl√©mente un pipeline complet de donn√©es, allant du scraping de sites e-commerce √† l'analyse des donn√©es avec Spark, en passant par un nettoyage ETL et un stockage dans Cassandra.

## üìã Pr√©requis

Avant de commencer, assurez-vous d'avoir install√© les outils suivants :

*   **Python 3.11** : Pour les scripts de scraping et d'analyse Spark.
*   **Docker & Docker Compose** : Pour lancer l'infrastructure (Cassandra).
*   **Pentaho Data Integration (PDI)** : Pour ex√©cuter les jobs ETL de nettoyage.
*   **Java (JDK 17 recommand√©)** : N√©cessaire pour ex√©cuter Spark et Pentaho.

## üöÄ Installation

1.  Clonez ce d√©p√¥t :
    ```bash
    git clone <votre-repo-url>
    cd web-scraping-ecommerce
    ```
2.  Cr√©ez un environnement virtuel Python :
    ```bash
    python3.11 -m venv venv
    source venv/bin/activate  # Sur Linux/Mac
    .\venv\Scripts\activate  # Sur Windows
    ```

3.  Installez les d√©pendances Python :
    ```bash
    pip install -r requirements.txt
    ```

## Utilisation du Pipeline

Suivez ces √©tapes dans l'ordre pour ex√©cuter le projet de bout en bout.

### 1. Scraping des Donn√©es

Lancez le script Python pour r√©cup√©rer les donn√©es des sites e-commerce configur√©s (Manojia, Jumia, Expat-Dakar).

```bash
python script_scraping.py
```

*   **R√©sultat** : Un fichier brut est g√©n√©r√© dans `data/produits.csv`.

### 2. Nettoyage des Donn√©es (ETL)

Utilisez **Pentaho Data Integration** pour nettoyer et transformer les donn√©es brutes.

1.  Ouvrez Pentaho (Spoon).
2.  Ouvrez le fichier `job_etl.kjb` (ou la transformation `Transformation 1.ktr` si vous souhaitez ex√©cuter uniquement la transformation).
3.  Ex√©cutez le job.

*   **R√©sultat** : Un fichier propre est g√©n√©r√© dans `output/produits_clean.csv`.

### 3. Lancement de l'Infrastructure

D√©marrez le conteneur Cassandra √† l'aide de Docker Compose.

```bash
docker-compose up -d
```

Attendez quelques secondes que le conteneur soit compl√®tement op√©rationnel.

### 4. Configuration de la Base de Donn√©es (Cassandra)

Une fois Cassandra lanc√©, vous devez cr√©er le sch√©ma (Keyspace et Table) et importer les donn√©es nettoy√©es.

Ex√©cutez la commande suivante pour appliquer le script `schema.cql` √† l'int√©rieur du conteneur :

```bash
docker exec -it cassandra-lab cqlsh -f /schema.cql
```

*   **Action** : Cette commande cr√©e le keyspace `ecommerce`, la table `produits`, et importe les donn√©es depuis `/data/produits_clean.csv` (mont√© via Docker).

### 5. Analyse des Donn√©es (Spark)

Enfin, lancez le script Spark pour effectuer des analyses sur les donn√©es stock√©es dans Cassandra.

```bash
python spark.py
```

Le script affichera dans la console :
*   Un aper√ßu des donn√©es.
*   Le prix moyen par vendeur.
*   Le prix moyen par cat√©gorie.
*   Le nombre de produits par cat√©gorie.

---

## Structure du Projet

*   `script_scraping.py` : Script de collecte de donn√©es.
*   `data/` : Dossier contenant les donn√©es brutes (`produits.csv`).
*   `Transformation 1.ktr` / `job_etl.kjb` : Fichiers ETL Pentaho.
*   `output/` : Dossier contenant les donn√©es nettoy√©es (`produits_clean.csv`).
*   `docker-compose.yml` : Configuration Docker pour Cassandra.
*   `schema.cql` : Script CQL pour la structure de la base de donn√©es et l'import.
*   `spark.py` : Script d'analyse PySpark connect√© √† Cassandra.
